# Behavior: LLM-Assisted Commenting

behavior: "LLM Integration for Assisted Commenting"
user_goal: "Get AI assistance for writing, reviewing, or commenting on document content"
priority: high

scenarios:
  - name: "Request LLM help on selected text"
    given:
      - "Document is open"
      - "User has selected text: 'function processPayment(amount)'"
      - "LLM provider (Claude) is configured"
      - "API credentials are valid"
    when:
      - "User triggers LLM help command"
      - "User query: 'Suggest improvements for this function'"
    then:
      - "Context builder gathers selection"
      - "Surrounding lines (Â±5) included for context"
      - "Relevant nearby comments included"
      - "Context assembled with query"
      - "Token count calculated"
      - "Request sent to Claude API"
      - "Loading indicator shown"
      - "Context size: 342 tokens"

  - name: "Stream LLM response"
    given:
      - "LLM request submitted successfully"
      - "Response started streaming"
    when:
      - "Chunks arrive from API"
    then:
      - "First chunk arrives within 2 seconds"
      - "Each chunk displayed immediately in preview pane"
      - "Preview pane shows cumulative response"
      - "Streaming progress indicator active"
      - "User can cancel at any time"
      - "Response builds progressively"
      - "Markdown formatting applied in real-time"
      - "Code blocks highlighted"
      - "No UI blocking during stream"

  - name: "Complete LLM response"
    given:
      - "LLM streaming in progress"
    when:
      - "Final chunk received"
      - "Response text: 'Consider adding input validation:\\n1. Check amount > 0\\n2. Validate currency\\n3. Add error handling'"
    then:
      - "Stream marked as complete"
      - "Full response displayed in preview"
      - "Action buttons appear: [Approve] [Edit] [Reject]"
      - "Response metadata shown (tokens used, time)"
      - "Preview remains in separate pane"
      - "User can review before decision"

  - name: "Approve LLM suggestion as comment"
    given:
      - "LLM response complete in preview"
      - "Response contains helpful suggestion"
      - "User has reviewed response"
    when:
      - "User clicks 'Approve' button"
      - "Selects 'Insert as comment' option"
    then:
      - "New comment created at selection position"
      - "Comment text contains LLM response"
      - "Comment author marked as 'AI Assistant' or similar"
      - "Comment metadata includes LLM source"
      - "Comment inserted into document"
      - "Preview pane closes"
      - "UI returns to normal view"
      - "Comment visible in comment panel"

  - name: "Edit LLM response before inserting"
    given:
      - "LLM response complete"
      - "Response text needs modification"
    when:
      - "User clicks 'Edit' button"
      - "User modifies response text"
      - "User approves edited version"
    then:
      - "Editable view shown with response text"
      - "User can modify content freely"
      - "Modified text becomes comment content"
      - "Comment author reflects 'AI (edited by user)'"
      - "Comment inserted at position"
      - "Original LLM response not stored"

  - name: "Reject LLM suggestion"
    given:
      - "LLM response complete"
      - "Response not helpful or incorrect"
    when:
      - "User clicks 'Reject' button"
    then:
      - "Preview pane closes"
      - "No comment created"
      - "No document modification"
      - "UI returns to previous state"
      - "Context preserved if user wants to retry"
      - "User can submit new query"

  - name: "LLM request without selection"
    given:
      - "No text selected"
      - "Cursor at line 45"
    when:
      - "User requests LLM help"
      - "Query: 'Explain this function'"
    then:
      - "Context builder uses cursor position"
      - "Current function or block identified"
      - "Surrounding context gathered intelligently"
      - "Request proceeds with expanded context"
      - "Response relates to cursor context"

  - name: "LLM request with comment context"
    given:
      - "Selected text has 3 existing comments"
      - "Comments contain questions and discussion"
    when:
      - "User requests LLM help"
      - "Query: 'Address the concerns in the comments'"
    then:
      - "Context includes selected text"
      - "All 3 comments included in context"
      - "Comment authors and timestamps included"
      - "Thread relationships preserved"
      - "LLM response addresses comment concerns"
      - "Response relevant to discussion"

  - name: "LLM provider failure"
    given:
      - "Request sent to primary provider (Claude)"
    when:
      - "Network error occurs"
      - "Connection timeout after 30 seconds"
    then:
      - "Error detected"
      - "Retry attempted with exponential backoff"
      - "After 3 retries, fallback provider tried"
      - "User notified of provider switch"
      - "Request continues with fallback"
      - "Graceful degradation maintained"

  - name: "LLM rate limit exceeded"
    given:
      - "User has made 10 requests in past minute"
      - "Rate limit is 10 per minute"
    when:
      - "User submits 11th request"
    then:
      - "Error code RATE_LIMITED returned"
      - "Clear message: 'Rate limit exceeded'"
      - "Time until rate limit resets shown"
      - "Request queued automatically"
      - "User can cancel queued request"
      - "Request proceeds when limit resets"

  - name: "LLM context too large"
    given:
      - "User selected 5000 lines of code"
      - "Selection would create 20,000 token context"
      - "Provider limit is 8,000 tokens"
    when:
      - "Context building attempted"
    then:
      - "Context size calculated: 20,000 tokens"
      - "Error: 'Context exceeds token limit'"
      - "Suggestion to reduce selection"
      - "Smart truncation offered"
      - "User can approve truncation"
      - "Or cancel and adjust selection"

  - name: "Cancel streaming response"
    given:
      - "LLM response actively streaming"
      - "User realizes request was wrong"
    when:
      - "User clicks 'Cancel' button"
    then:
      - "Stream immediately interrupted"
      - "API request cancelled if possible"
      - "Partial response discarded"
      - "Preview pane closes"
      - "UI returns to normal"
      - "No side effects from cancellation"

  - name: "Multiple LLM requests in parallel"
    given:
      - "User has submitted request R1"
      - "R1 is streaming"
    when:
      - "User submits second request R2"
    then:
      - "R2 queued until R1 completes"
      - "User notified of queue status"
      - "Or R1 cancelled automatically"
      - "R2 proceeds after queue clears"
      - "No interference between requests"

  - name: "LLM suggestion as inline text"
    given:
      - "LLM response is code suggestion"
      - "Response: 'Replace with: if (amount > 0) { ... }'"
    when:
      - "User approves as 'Insert as text'"
    then:
      - "Response content inserted directly"
      - "Not wrapped in comment markup"
      - "Inserted at cursor position"
      - "Original selection replaced if any"
      - "Edit operation tracked"
      - "Position updates triggered"

edge_cases:
  - name: "LLM returns empty response"
    condition: "API returns empty string"
    expected: "Error shown, user can retry"

  - name: "LLM returns malformed markdown"
    condition: "Response has broken syntax"
    expected: "Displayed as-is, still usable"

  - name: "Network interruption mid-stream"
    condition: "Connection lost during streaming"
    expected: "Error detected, partial response saveable, retry offered"

  - name: "LLM request during save"
    condition: "Document saving while LLM processing"
    expected: "Both operations succeed independently"

  - name: "Extremely long LLM response"
    condition: "Response is 5000 lines"
    expected: "Scrollable preview, chunked display, memory efficient"

  - name: "LLM response with code in multiple languages"
    condition: "Response has Python, JavaScript, SQL"
    expected: "Each block syntax-highlighted correctly"

context_building_rules:
  selection_mode:
    - "Include exact selection"
    - "Add N lines before/after (configurable)"
    - "Include comments within/near selection"

  cursor_mode:
    - "Identify current block (function, paragraph)"
    - "Include complete block"
    - "Add surrounding context"

  document_mode:
    - "Include document metadata"
    - "Include relevant comments"
    - "Smart truncation if too large"

provider_configuration:
  primary: "claude"
  fallback: ["openai", "local"]
  timeouts:
    - connect: 10_seconds
    - first_byte: 30_seconds
    - total: 120_seconds
  retry_policy:
    - max_retries: 3
    - backoff: exponential
    - max_delay: 30_seconds

response_handling:
  streaming:
    - "Display chunks as received"
    - "Buffer for smooth display"
    - "Handle backpressure"

  formatting:
    - "Parse markdown syntax"
    - "Highlight code blocks"
    - "Render lists and tables"

  metadata:
    - "Track tokens used"
    - "Record processing time"
    - "Store provider info"

invariants:
  - "Context never exceeds provider token limit"
  - "User can always cancel request"
  - "Streaming never blocks UI"
  - "Response always complete or error state"
  - "No partial insertions on failure"
  - "API credentials never logged"

quality_attributes:
  usability:
    - "Response starts streaming within 2 seconds"
    - "Clear indication of LLM activity"
    - "Easy to approve/reject/edit"
    - "Preview pane doesn't obstruct document"

  reliability:
    - "Graceful degradation on errors"
    - "Fallback providers work"
    - "No data loss on cancellation"

  performance:
    - "Streaming smooth and responsive"
    - "Context building < 100ms"
    - "UI remains responsive"
    - "Memory efficient for long responses"
