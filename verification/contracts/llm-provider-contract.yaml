# Contract Tests: LLM Provider Interface

component: "LLM Provider Adapter"
interface: "RequestLLMCompletion"
description: "Validates LLM provider integration contracts"

# Create Context Contract
create_context:
  operation: "CreateLLMContext"

  input_schema:
    document_id: uuid
    selection: Position?
    include_comments: boolean?
    context_lines: integer?
    user_query: string

  output_schema:
    success:
      context_id: uuid
      context_size: integer  # in tokens
      included_comments: integer
      preview: string

    error:
      code: enum[DOCUMENT_NOT_FOUND, CONTEXT_TOO_LARGE]
      message: string

  scenarios:
    - name: "Build context with selection"
      input:
        document_id: "{doc_id}"
        selection: {start: 100, end: 200}
        include_comments: true
        context_lines: 5
        user_query: "Explain this code"
      expect:
        success: true
        context_id: "not_null"
        context_size: "> 0"
        preview_contains:
          - selected_text
          - surrounding_lines
          - nearby_comments

    - name: "Context without selection"
      input:
        document_id: "{doc_id}"
        selection: null
        user_query: "General question"
      expect:
        success: true
        context_includes:
          - document_metadata
          - cursor_position_context

    - name: "Context exceeds token limit"
      input:
        document_id: "{large_doc}"
        selection: {start: 0, end: 999999}
      expect:
        error: true
        error_code: "CONTEXT_TOO_LARGE"
        suggestion: "reduce selection"

    - name: "Include relevant comments"
      setup:
        - document_has_comments: 10
        - selection_has_nearby_comments: 3
      input:
        include_comments: true
      expect:
        included_comments: 3
        comments_in_preview: true

    - name: "Exclude comments"
      input:
        include_comments: false
      expect:
        included_comments: 0
        only_document_content: true

# Request Completion Contract
request_completion:
  operation: "RequestLLMCompletion"

  input_schema:
    context_id: uuid
    provider: enum[claude, openai, local, custom]
    model: string
    temperature: float?
    max_tokens: integer?
    stream: boolean?

  output_schema:
    success:
      response_id: uuid
      streaming: boolean
      estimated_time: integer  # milliseconds

    error:
      code: enum[PROVIDER_ERROR, RATE_LIMITED, INVALID_CREDENTIALS]
      message: string

  scenarios:
    - name: "Successful streaming request"
      input:
        context_id: "{ctx_id}"
        provider: "claude"
        model: "claude-3-sonnet"
        stream: true
      expect:
        success: true
        response_id: "not_null"
        streaming: true
        estimated_time: "> 0"

    - name: "Successful non-streaming request"
      input:
        context_id: "{ctx_id}"
        provider: "openai"
        model: "gpt-4"
        stream: false
      expect:
        success: true
        streaming: false

    - name: "Rate limit exceeded"
      setup:
        - requests_in_minute: 10
        - rate_limit: 10
      input:
        context_id: "{ctx_id}"
        provider: "claude"
      expect:
        error: true
        error_code: "RATE_LIMITED"
        retry_after: "> 0"
        message_contains: "rate limit"

    - name: "Invalid API credentials"
      setup:
        - api_key: "invalid"
      input:
        context_id: "{ctx_id}"
        provider: "claude"
      expect:
        error: true
        error_code: "INVALID_CREDENTIALS"
        message: "Authentication failed"

    - name: "Provider unavailable"
      setup:
        - network_error: true
      input:
        context_id: "{ctx_id}"
        provider: "claude"
      expect:
        error: true
        error_code: "PROVIDER_ERROR"
        retry_suggested: true

    - name: "Fallback to secondary provider"
      setup:
        - primary_provider: "claude"
        - primary_fails: true
        - fallback_provider: "openai"
      input:
        provider: "claude"
      expect:
        fallback_activated: true
        final_provider: "openai"
        user_notified: true

    - name: "Custom temperature"
      input:
        temperature: 0.7
      expect:
        success: true
        temperature_applied: 0.7

    - name: "Custom max_tokens"
      input:
        max_tokens: 1000
      expect:
        success: true
        max_tokens_applied: 1000

    - name: "Invalid temperature"
      input:
        temperature: 3.0  # > 2.0
      expect:
        error_or_clamped: true

    - name: "Context not found"
      input:
        context_id: "invalid-id"
      expect:
        error: true
        message: "Context not found"

# Stream Response Contract
stream_response:
  operation: "StreamLLMResponse"

  input_schema:
    response_id: uuid

  output_schema:
    success:
      chunk: string
      index: integer
      finished: boolean
      total_tokens: integer?

    error:
      code: enum[STREAM_INTERRUPTED, RESPONSE_NOT_FOUND]
      message: string

  scenarios:
    - name: "Receive streaming chunks"
      input:
        response_id: "{resp_id}"
      expect:
        chunks_arrive: true
        index_sequential: true
        finished_false: true

    - name: "Final chunk marks completion"
      input:
        response_id: "{resp_id}"
        chunk_number: "final"
      expect:
        chunk: "last text"
        finished: true
        total_tokens: "> 0"

    - name: "Stream interrupted"
      setup:
        - start_streaming: true
        - network_fails: true
      expect:
        error: true
        error_code: "STREAM_INTERRUPTED"
        partial_content_available: true

    - name: "Response not found"
      input:
        response_id: "invalid"
      expect:
        error: true
        error_code: "RESPONSE_NOT_FOUND"

    - name: "Chunks arrive in order"
      verification:
        - receive_chunk_1: {index: 0}
        - receive_chunk_2: {index: 1}
        - receive_chunk_3: {index: 2}
      expect:
        index_monotonic: true
        no_gaps: true

    - name: "Empty chunks handled"
      input:
        chunk: ""
      expect:
        accepted: true
        no_error: true

    - name: "Chunk with special characters"
      input:
        chunk: "Code: `{>>` and emoji ðŸŽ‰"
      expect:
        text_preserved: true
        no_encoding_issues: true

# Response Approval Contract
approve_response:
  operation: "ApproveLLMResponse"

  input_schema:
    response_id: uuid
    insert_as: enum[comment, text, suggestion]
    position: Position?
    author_override: string?

  output_schema:
    success:
      inserted_id: string
      final_content: string

    error:
      code: enum[RESPONSE_NOT_READY, RESPONSE_EXPIRED]
      message: string

  scenarios:
    - name: "Approve as comment"
      setup:
        - response_complete: true
        - response_text: "Suggestion text"
      input:
        response_id: "{resp_id}"
        insert_as: "comment"
        position: {line: 10, column: 0}
      expect:
        success: true
        inserted_id: "comment_id"
        comment_created: true
        comment_text: "Suggestion text"

    - name: "Approve as inline text"
      input:
        insert_as: "text"
        position: {line: 15, column: 5}
      expect:
        success: true
        text_inserted: true
        no_comment_wrapper: true

    - name: "Approve with author override"
      input:
        insert_as: "comment"
        author_override: "AI (edited)"
      expect:
        comment_author: "AI (edited)"

    - name: "Approve before streaming complete"
      setup:
        - response_streaming: true
        - not_finished: true
      input:
        response_id: "{resp_id}"
      expect:
        error: true
        error_code: "RESPONSE_NOT_READY"
        message: "still streaming"

    - name: "Approve expired response"
      setup:
        - response_completed: "30 minutes ago"
        - response_expired: true
      input:
        response_id: "{resp_id}"
      expect:
        error: true
        error_code: "RESPONSE_EXPIRED"

# Provider-Specific Contracts
provider_claude:
  name: "Claude API Adapter"

  authentication:
    - api_key: "required"
    - header: "x-api-key"

  request_format:
    - method: "POST"
    - endpoint: "/v1/messages"
    - body:
        model: string
        messages: array
        max_tokens: integer
        stream: boolean

  response_format:
    - streaming:
        events: "server-sent events"
        event_types: ["message_start", "content_block_delta", "message_stop"]
    - non_streaming:
        json_body:
          content: array
          usage: object

  scenarios:
    - name: "Claude streaming request"
      expect:
        sse_connection: true
        event_stream_parsed: true

    - name: "Claude rate limit response"
      response:
        status: 429
        headers:
          retry-after: 60
      expect:
        rate_limit_detected: true
        retry_after_extracted: 60

provider_openai:
  name: "OpenAI API Adapter"

  authentication:
    - api_key: "required"
    - header: "Authorization: Bearer"

  request_format:
    - method: "POST"
    - endpoint: "/v1/chat/completions"
    - body:
        model: string
        messages: array
        max_tokens: integer
        stream: boolean

  response_format:
    - streaming:
        events: "server-sent events"
        data_prefix: "data: "
    - non_streaming:
        json_body:
          choices: array
          usage: object

provider_local:
  name: "Local Model Adapter"

  scenarios:
    - name: "Local model request"
      expect:
        no_network_call: true
        local_inference: true

    - name: "Local model not available"
      setup:
        - model_not_loaded: true
      expect:
        error: "Model not available"
        suggestion: "Load model or use remote provider"

# Timeout Handling
timeouts:
  - name: "Connection timeout"
    setup:
      - timeout: 10_seconds
      - no_connection: true
    expect:
      error_within: 10_seconds
      error_code: "PROVIDER_ERROR"

  - name: "First byte timeout"
    setup:
      - connected: true
      - no_response: true
      - timeout: 30_seconds
    expect:
      timeout_triggered: true
      connection_closed: true

  - name: "Total request timeout"
    setup:
      - max_duration: 120_seconds
    expect:
      request_cancelled_after: 120_seconds

# Retry Logic
retry_behavior:
  - name: "Network error retry"
    setup:
      - network_error: true
      - max_retries: 3
    expect:
      retry_count: 3
      backoff: "exponential"
      delays: [1s, 2s, 4s]

  - name: "Rate limit no retry"
    setup:
      - rate_limited: true
    expect:
      no_immediate_retry: true
      queued_for_later: true

  - name: "Auth error no retry"
    setup:
      - auth_failed: true
    expect:
      no_retry: true
      immediate_error: true

# Performance
performance:
  - name: "Context building speed"
    input:
      document_size: 1MB
      selection_size: 1KB
    expect:
      build_time: "< 100ms"

  - name: "First chunk latency"
    expect:
      first_chunk: "< 2 seconds"

  - name: "Streaming smooth"
    expect:
      chunk_frequency: "> 10 chunks/second"
      no_buffering_delays: true

# Invariants
invariants:
  - "Context size never exceeds provider limit"
  - "Stream chunks arrive in order"
  - "Response ID unique per request"
  - "Streaming boolean matches actual behavior"
  - "API credentials never logged"
  - "Timeout always enforced"
  - "Retry count respects maximum"
  - "Token count accurate when provided"

# State Machine
response_states:
  states: [pending, streaming, complete, failed, cancelled, expired]

  transitions:
    - from: pending
      to: streaming
      trigger: "first_chunk"

    - from: streaming
      to: complete
      trigger: "final_chunk"

    - from: [pending, streaming]
      to: failed
      trigger: "error"

    - from: [pending, streaming]
      to: cancelled
      trigger: "user_cancel"

    - from: complete
      to: expired
      trigger: "timeout_elapsed"
